{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import logging\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act(act):\n",
    "    if (act == 'sigmoid'):\n",
    "        return sp.special.expit\n",
    "    elif (act == 'tanh'):\n",
    "        return np.tanh\n",
    "    elif (act == 'relu'):\n",
    "        return lambda x: np.maximum(x, 0)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "weight_names = ['fc1_weight', 'fc2_weight']\n",
    "bias_names = ['fc1_bias', 'fc2_bias']\n",
    "\n",
    "def plot_errors(x, y):\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "class AutoEncoderModel:\n",
    "    def __init__(self, data, num_dims, internal_act=None, output_act=None, learning_rate=0.005, batch_size=50):\n",
    "        dims = [data.shape[1], num_dims, data.shape[1]]\n",
    "        self.data = mx.symbol.Variable('data')\n",
    "        self.y = mx.symbol.Variable('label')\n",
    "        self.fc1_weight = mx.symbol.Variable(weight_names[0])\n",
    "        self.fc1_bias = mx.symbol.Variable(bias_names[0])\n",
    "        self.fc2_weight = mx.symbol.Variable(weight_names[1])\n",
    "        self.fc2_bias = mx.symbol.Variable(bias_names[1])\n",
    "        x = mx.symbol.FullyConnected(data=self.data, weight=self.fc1_weight,\n",
    "                                     bias=self.fc1_bias, num_hidden=dims[1])\n",
    "        if (internal_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=internal_act)\n",
    "            print(\"Internal activation: \" + internal_act)\n",
    "        self.layer1 = x\n",
    "        x = mx.symbol.FullyConnected(data=x, weight=self.fc2_weight,\n",
    "                                     bias=self.fc2_bias, num_hidden=dims[2])\n",
    "        if (output_act is not None):\n",
    "            x = mx.symbol.Activation(data=x, act_type=output_act)\n",
    "            print(\"Output activation: \" + output_act)\n",
    "        self.layer2 = x\n",
    "        # TODO How about using L1/L2 regularization.\n",
    "        self.loss = mx.symbol.LinearRegressionOutput(data=x, label=self.y)\n",
    "        self.model = mx.mod.Module(symbol=self.loss, data_names=['data'], label_names = ['label'])\n",
    "        self.init_data(data, batch_size, learning_rate)\n",
    "        \n",
    "        def cal_model_numpy(params):\n",
    "            fc1_weight = params.get(weight_names[0]).asnumpy()\n",
    "            fc1_bias = params.get(bias_names[0]).asnumpy()\n",
    "            fc2_weight = params.get(weight_names[1]).asnumpy()\n",
    "            fc2_bias = params.get(bias_names[1]).asnumpy()\n",
    "\n",
    "            np_data = data.asnumpy()\n",
    "            hidden = np.dot(np_data, fc1_weight.T) + fc1_bias\n",
    "            act_func = get_act(internal_act)\n",
    "            if (act_func is not None):\n",
    "                hidden = act_func(hidden)\n",
    "            output = np.dot(hidden, fc2_weight.T) + fc2_bias\n",
    "            act_func = get_act(output_act)\n",
    "            if (act_func is not None):\n",
    "                output = act_func(output)\n",
    "            return np.sum(np.square(output - np_data))\n",
    "\n",
    "        self.numpy_cal = cal_model_numpy\n",
    "        \n",
    "    def init_data(self, data, batch_size=50, learning_rate=0.005):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        data_iter = mx.io.NDArrayIter(data={'data':data}, label={'label':data},\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                last_batch_handle='roll_over')\n",
    "        print(\"Learning rate: \" + str(learning_rate))\n",
    "        print(\"batch size: \" + str(batch_size))\n",
    "        # allocate memory given the input data and label shapes\n",
    "        self.model.bind(data_shapes=data_iter.provide_data, label_shapes=data_iter.provide_label)\n",
    "        # initialize parameters by uniform random numbers\n",
    "        self.model.init_params(initializer=mx.init.Uniform(scale=.1))\n",
    "        # use SGD with learning rate 0.1 to train\n",
    "        self.model.init_optimizer(optimizer='sgd',\n",
    "                                  optimizer_params={'learning_rate': learning_rate,\n",
    "                                                    'momentum': 0.9})\n",
    "\n",
    "    def fit_int(self, data, batch_size, num_epoch, params=None, learning_rate=0.005, reinit_opt=True):\n",
    "        data_iter = mx.io.NDArrayIter(data={'data':data}, label={'label':data},\n",
    "                batch_size=batch_size, shuffle=True,\n",
    "                last_batch_handle='roll_over')\n",
    "        \n",
    "        if (params is not None):\n",
    "            self.model.set_params(arg_params=params, aux_params=None, force_init=True)\n",
    "            if (reinit_opt):\n",
    "                print(\"reinit optimizer. New learning rate: \" + str(learning_rate))\n",
    "                self.model.init_optimizer(optimizer='sgd',\n",
    "                                          optimizer_params={'learning_rate': learning_rate,\n",
    "                                                            'momentum': 0.9}, force_init=True)\n",
    "        # use accuracy as the metric\n",
    "        metric = mx.metric.create('acc')\n",
    "        # train 5 epochs, i.e. going over the data iter one pass\n",
    "        for epoch in range(num_epoch):\n",
    "            data_iter.reset()\n",
    "            metric.reset()\n",
    "            for batch in data_iter:\n",
    "                self.model.forward(batch, is_train=True)       # compute predictions\n",
    "                self.model.update_metric(metric, batch.label)  # accumulate prediction accuracy\n",
    "                self.model.backward()                          # compute gradients\n",
    "                self.model.update()                            # update parameters\n",
    "            #print('Epoch %d, Training %s' % (epoch, metric.get()))\n",
    "\n",
    "    def train(self, data, num_epoc, params = None, debug=False, return_err=False):\n",
    "        print(\"internal #epochs: \" + str(num_epoc))\n",
    "        int_epoc = 100\n",
    "        prev_val = None\n",
    "        reinit_opt = True\n",
    "        plot_xs = []\n",
    "        plot_yx = []\n",
    "        for i in range(num_epoc/int_epoc):\n",
    "            curr = time.time()\n",
    "            self.fit_int(data, self.batch_size, int_epoc, params, self.learning_rate, reinit_opt=reinit_opt)\n",
    "            if (debug):\n",
    "                print(str(int_epoc) + \" epochs takes \" + str(time.time() - curr) + \" seconds\")\n",
    "            reinit_opt = False\n",
    "\n",
    "            params = self.model.get_params()[0]\n",
    "            val = self.numpy_cal(params)\n",
    "            plot_xs.append((i + 1) * int_epoc)\n",
    "            plot_yx.append(val)\n",
    "            if (debug):\n",
    "                print(\"epoc \" + str((i + 1) * int_epoc) + \": \" + str(val))\n",
    "                sys.stdout.flush()\n",
    "            if (prev_val is not None and prev_val < val):\n",
    "                self.learning_rate = self.learning_rate / 2\n",
    "                reinit_opt = True\n",
    "            prev_val = val\n",
    "        plot_errors(plot_xs, plot_yx)\n",
    "        if (return_err):\n",
    "            return params, plot_xs, plot_yx\n",
    "        else:\n",
    "            return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, num_dims, num_epoc, internal_act=None, output_act=None, learning_rate=0.005, batch_size=50, debug=False, return_err=False):\n",
    "    model = AutoEncoderModel(data, num_dims, internal_act, output_act, learning_rate, batch_size)\n",
    "    return model.train(data, num_epoc, debug=debug, return_err=return_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a low-rank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: \n",
      "[ 5.49624777]\n",
      "<NDArray 1 @cpu(0)>\n",
      "(1000L, 100L)\n"
     ]
    }
   ],
   "source": [
    "rand_data1 = mx.ndarray.random_uniform(shape=[1000, 10])\n",
    "rand_data2 = mx.ndarray.random_uniform(shape=[10, 100])\n",
    "rand_data = mx.ndarray.dot(rand_data1, rand_data2)\n",
    "print(\"max: \" + str(mx.ndarray.max(rand_data)))\n",
    "rand_data = rand_data / mx.ndarray.max(rand_data)\n",
    "print(rand_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06789682  0.11745708  0.01811091 ..., -0.22041818  0.25947556\n",
      "   4.58424139]\n",
      " [ 0.06577194  0.15134373  0.12103066 ...,  0.09472218  0.52994013\n",
      "   4.72747707]\n",
      " [ 0.18843757 -0.02188844  0.01198309 ..., -0.19646543  0.0452216\n",
      "   3.51541185]\n",
      " ..., \n",
      " [ 0.14157917  0.11198635 -0.05527457 ..., -0.25732851  0.19383214\n",
      "   4.35923386]\n",
      " [ 0.04307616 -0.11992847 -0.01043215 ..., -0.1986502  -0.17690027\n",
      "   5.00072002]\n",
      " [-0.03079219  0.05760233 -0.08055174 ...,  0.09610062 -0.04332777\n",
      "   3.86204791]]\n",
      "5186.59034291\n",
      "-553.564527681\n",
      "svd error: 1.31856e-08\n"
     ]
    }
   ],
   "source": [
    "np_rand_data = rand_data.asnumpy()\n",
    "U, s, Vh = sp.sparse.linalg.svds(np_rand_data, k=10)\n",
    "low_dim_data = np.dot(np_rand_data, Vh.T)\n",
    "print(low_dim_data)\n",
    "print(sum(low_dim_data[low_dim_data > 0]))\n",
    "print(sum(low_dim_data[low_dim_data < 0]))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_rand_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.4\n",
      "batch size: 100\n",
      "internal #epochs: 5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKyg/w8ny1wpWJ49ygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe18bd59a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_linear_r10=train(rand_data, 10, 5000, learning_rate=0.4, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.4\n",
      "batch size: 100\n",
      "internal #epochs: 5000\n",
      "100 epochs takes 1.29150795937 seconds\n",
      "epoc 100: 152.982\n",
      "100 epochs takes 1.27169084549 seconds\n",
      "epoc 200: 135.442\n",
      "100 epochs takes 1.31027507782 seconds\n",
      "epoc 300: 105.656\n",
      "100 epochs takes 1.30603003502 seconds\n",
      "epoc 400: 74.7395\n",
      "100 epochs takes 1.29361391068 seconds\n",
      "epoc 500: 53.4827\n",
      "100 epochs takes 1.27092599869 seconds\n",
      "epoc 600: 38.1661\n",
      "100 epochs takes 1.36336493492 seconds\n",
      "epoc 700: 27.6645\n",
      "100 epochs takes 1.35749292374 seconds\n",
      "epoc 800: 20.4129\n",
      "100 epochs takes 1.29214310646 seconds\n",
      "epoc 900: 15.0069\n",
      "100 epochs takes 1.28233599663 seconds\n",
      "epoc 1000: 10.7732\n",
      "100 epochs takes 1.25471997261 seconds\n",
      "epoc 1100: 7.34073\n",
      "100 epochs takes 1.32835412025 seconds\n",
      "epoc 1200: 4.63181\n",
      "100 epochs takes 1.24992394447 seconds\n",
      "epoc 1300: 2.7774\n",
      "100 epochs takes 1.37860703468 seconds\n",
      "epoc 1400: 1.77053\n",
      "100 epochs takes 1.37559890747 seconds\n",
      "epoc 1500: 1.21751\n",
      "100 epochs takes 1.24903512001 seconds\n",
      "epoc 1600: 0.967428\n",
      "100 epochs takes 1.33065295219 seconds\n",
      "epoc 1700: 0.840683\n",
      "100 epochs takes 1.32698178291 seconds\n",
      "epoc 1800: 0.768694\n",
      "100 epochs takes 1.31023907661 seconds\n",
      "epoc 1900: 0.77611\n",
      "reinit optimizer. New learning rate: 0.2\n",
      "100 epochs takes 1.28801512718 seconds\n",
      "epoc 2000: 0.706114\n",
      "100 epochs takes 1.17169713974 seconds\n",
      "epoc 2100: 0.691267\n",
      "100 epochs takes 1.28235602379 seconds\n",
      "epoc 2200: 0.68158\n",
      "100 epochs takes 1.27048516273 seconds\n",
      "epoc 2300: 0.671656\n",
      "100 epochs takes 1.34652996063 seconds\n",
      "epoc 2400: 0.655606\n",
      "100 epochs takes 1.41795992851 seconds\n",
      "epoc 2500: 0.654552\n",
      "100 epochs takes 1.285145998 seconds\n",
      "epoc 2600: 0.634406\n",
      "100 epochs takes 1.33557891846 seconds\n",
      "epoc 2700: 0.626106\n",
      "100 epochs takes 1.31165099144 seconds\n",
      "epoc 2800: 0.644731\n",
      "reinit optimizer. New learning rate: 0.1\n",
      "100 epochs takes 1.34507513046 seconds\n",
      "epoc 2900: 0.611991\n",
      "100 epochs takes 1.29152703285 seconds\n",
      "epoc 3000: 0.607961\n",
      "100 epochs takes 1.37127900124 seconds\n",
      "epoc 3100: 0.604166\n",
      "100 epochs takes 1.24881100655 seconds\n",
      "epoc 3200: 0.59973\n",
      "100 epochs takes 1.34056019783 seconds\n",
      "epoc 3300: 0.595895\n",
      "100 epochs takes 1.30366110802 seconds\n",
      "epoc 3400: 0.591916\n",
      "100 epochs takes 1.32856798172 seconds\n",
      "epoc 3500: 0.588126\n",
      "100 epochs takes 1.3317668438 seconds\n",
      "epoc 3600: 0.585766\n",
      "100 epochs takes 1.25145792961 seconds\n",
      "epoc 3700: 0.581847\n",
      "100 epochs takes 1.37125301361 seconds\n",
      "epoc 3800: 0.579343\n",
      "100 epochs takes 1.28461194038 seconds\n",
      "epoc 3900: 0.57346\n",
      "100 epochs takes 1.31471419334 seconds\n",
      "epoc 4000: 0.57039\n",
      "100 epochs takes 1.33229804039 seconds\n",
      "epoc 4100: 0.566537\n",
      "100 epochs takes 1.35250592232 seconds\n",
      "epoc 4200: 0.563156\n",
      "100 epochs takes 1.38973784447 seconds\n",
      "epoc 4300: 0.560845\n",
      "100 epochs takes 1.31917405128 seconds\n",
      "epoc 4400: 0.556868\n",
      "100 epochs takes 1.35428714752 seconds\n",
      "epoc 4500: 0.55382\n",
      "100 epochs takes 1.40992116928 seconds\n",
      "epoc 4600: 0.550546\n",
      "100 epochs takes 1.34460186958 seconds\n",
      "epoc 4700: 0.547747\n",
      "100 epochs takes 1.41108298302 seconds\n",
      "epoc 4800: 0.543451\n",
      "100 epochs takes 1.36708807945 seconds\n",
      "epoc 4900: 0.540811\n",
      "100 epochs takes 1.38513207436 seconds\n",
      "epoc 5000: 0.537369\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid_r10=train(rand_data, 10, 5000, internal_act='tanh', learning_rate=0.4, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on real data\n",
    "\n",
    "We compute the embedding on a graph with 81306 vertices and 1768149 vertices. To embed the graph into 10 dimensions, we start with the most densest columns and increase the number of columns to embed. When we increase the number of columns to embed, we use the parameters trained from the previous run (on the dataset with a smaller number of columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "elg = nx.read_edgelist(\"/home/ubuntu/datasets/twitter_combined.txt\")\n",
    "spm = nx.to_scipy_sparse_matrix(elg, dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_densest_idx(spm, num):\n",
    "    colsum = np.ravel(spm.sum(axis=0))\n",
    "    max_cols = np.sort(np.ravel(colsum), axis=None)[len(colsum) - num]\n",
    "    return max_cols, colsum >= max_cols\n",
    "\n",
    "def get_densest(spm, num):\n",
    "    max_cols, idx = get_densest_idx(spm, num)\n",
    "    sp_data = spm[:,idx]\n",
    "    return sp_data\n",
    "\n",
    "def get_densest2(spm, num1, num2):\n",
    "    colsum = np.ravel(spm.sum(axis=1))\n",
    "    sorted_colsum = np.sort(np.ravel(colsum), axis=None)\n",
    "    max_cols1 = sorted_colsum[len(colsum) - num1]\n",
    "    max_cols2 = sorted_colsum[len(colsum) - (num2 + num1)]\n",
    "    sp_data1 = spm[:,colsum >= max_cols1]\n",
    "    sp_data2 = spm[:,np.logical_and(colsum >= max_cols2, colsum < max_cols1)]\n",
    "    return sp_data1, sp_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 2490.  3383.  2484.  2758.  2476.  1789.  2133.  3011.  3239.  2155.]\n",
      "<NDArray 10 @cpu(0)>\n",
      "(81306L, 10L)\n"
     ]
    }
   ],
   "source": [
    "sp_data10 = get_densest(spm, 10)\n",
    "data10 = mx.ndarray.sparse.csr_matrix(sp_data10)\n",
    "print(mx.ndarray.sum(data10, axis=0))\n",
    "print(data10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81306, 10)\n",
      "1.21314\n",
      "-2.3992\n",
      "svd error: 6.78263e-09\n"
     ]
    }
   ],
   "source": [
    "np_data10 = data10.asnumpy()\n",
    "U, s, Vh = sp.linalg.svd(np_data10, full_matrices=False)\n",
    "low_dim_data = np.dot(np_data10, Vh.T)\n",
    "print(low_dim_data.shape)\n",
    "print(np.max(low_dim_data))\n",
    "print(np.min(low_dim_data))\n",
    "res = np.dot(low_dim_data, Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - np_data10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 6.57384419441 seconds\n",
      "epoc 0: 661.281\n",
      "100 epochs takes 7.22010612488 seconds\n",
      "epoc 100: 205.035\n",
      "100 epochs takes 6.69524002075 seconds\n",
      "epoc 200: 86.0281\n",
      "100 epochs takes 6.86633992195 seconds\n",
      "epoc 300: 69.2754\n",
      "100 epochs takes 7.26551198959 seconds\n",
      "epoc 400: 48.0851\n",
      "100 epochs takes 7.08142089844 seconds\n",
      "epoc 500: 24.3964\n",
      "100 epochs takes 6.72911000252 seconds\n",
      "epoc 600: 8.38904\n",
      "100 epochs takes 6.66133213043 seconds\n",
      "epoc 700: 2.06232\n",
      "100 epochs takes 6.76626992226 seconds\n",
      "epoc 800: 0.410246\n",
      "100 epochs takes 7.24766898155 seconds\n",
      "epoc 900: 0.0735736\n",
      "100 epochs takes 6.81751203537 seconds\n",
      "epoc 1000: 0.0125639\n",
      "100 epochs takes 7.10305809975 seconds\n",
      "epoc 1100: 0.00211213\n",
      "100 epochs takes 7.08962607384 seconds\n",
      "epoc 1200: 0.00035564\n",
      "100 epochs takes 6.72433185577 seconds\n",
      "epoc 1300: 6.57533e-05\n",
      "100 epochs takes 6.78700900078 seconds\n",
      "epoc 1400: 1.69345e-05\n",
      "100 epochs takes 6.82878088951 seconds\n",
      "epoc 1500: 1.13987e-05\n",
      "100 epochs takes 6.91550898552 seconds\n",
      "epoc 1600: 1.07326e-05\n",
      "100 epochs takes 6.56485104561 seconds\n",
      "epoc 1700: 1.04272e-05\n",
      "100 epochs takes 6.64296412468 seconds\n",
      "epoc 1800: 1.04248e-05\n",
      "100 epochs takes 7.0324318409 seconds\n",
      "epoc 1900: 1.0213e-05\n"
     ]
    }
   ],
   "source": [
    "params_linear10=train(data10, 10, 2000, internal_act=None, learning_rate=0.1, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Learning rate: 0.1\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 8.79215788841 seconds\n",
      "epoc 0: 782.973\n",
      "100 epochs takes 8.54049897194 seconds\n",
      "epoc 100: 365.227\n",
      "100 epochs takes 8.9551320076 seconds\n",
      "epoc 200: 177.58\n",
      "100 epochs takes 8.56133294106 seconds\n",
      "epoc 300: 121.924\n",
      "100 epochs takes 8.85677790642 seconds\n",
      "epoc 400: 115.84\n",
      "100 epochs takes 8.66231513023 seconds\n",
      "epoc 500: 110.476\n",
      "100 epochs takes 8.81901407242 seconds\n",
      "epoc 600: 102.967\n",
      "100 epochs takes 9.03215098381 seconds\n",
      "epoc 700: 91.0163\n",
      "100 epochs takes 8.80674505234 seconds\n",
      "epoc 800: 72.9704\n",
      "100 epochs takes 8.79361891747 seconds\n",
      "epoc 900: 51.2826\n",
      "100 epochs takes 8.95192408562 seconds\n",
      "epoc 1000: 33.1414\n",
      "100 epochs takes 8.9709789753 seconds\n",
      "epoc 1100: 22.8834\n",
      "100 epochs takes 8.84673404694 seconds\n",
      "epoc 1200: 18.4841\n",
      "100 epochs takes 9.01217198372 seconds\n",
      "epoc 1300: 16.636\n",
      "100 epochs takes 8.86660695076 seconds\n",
      "epoc 1400: 15.6374\n",
      "100 epochs takes 8.89191913605 seconds\n",
      "epoc 1500: 14.9088\n",
      "100 epochs takes 9.16901397705 seconds\n",
      "epoc 1600: 14.2892\n",
      "100 epochs takes 9.02341890335 seconds\n",
      "epoc 1700: 13.7308\n",
      "100 epochs takes 9.12369418144 seconds\n",
      "epoc 1800: 13.2201\n",
      "100 epochs takes 9.03474807739 seconds\n",
      "epoc 1900: 12.7497\n",
      "100 epochs takes 9.10241293907 seconds\n",
      "epoc 2000: 12.3147\n",
      "100 epochs takes 8.91687321663 seconds\n",
      "epoc 2100: 11.9099\n",
      "100 epochs takes 8.86173796654 seconds\n",
      "epoc 2200: 11.5329\n",
      "100 epochs takes 8.76117992401 seconds\n",
      "epoc 2300: 11.1801\n",
      "100 epochs takes 8.98178315163 seconds\n",
      "epoc 2400: 10.8497\n",
      "100 epochs takes 8.97622990608 seconds\n",
      "epoc 2500: 10.5398\n",
      "100 epochs takes 8.96583509445 seconds\n",
      "epoc 2600: 10.2481\n",
      "100 epochs takes 8.74402785301 seconds\n",
      "epoc 2700: 9.97309\n",
      "100 epochs takes 8.94157505035 seconds\n",
      "epoc 2800: 9.71337\n",
      "100 epochs takes 8.92731595039 seconds\n",
      "epoc 2900: 9.46753\n",
      "100 epochs takes 9.16012406349 seconds\n",
      "epoc 3000: 9.23465\n",
      "100 epochs takes 9.10220098495 seconds\n",
      "epoc 3100: 9.0137\n",
      "100 epochs takes 8.79331207275 seconds\n",
      "epoc 3200: 8.80388\n",
      "100 epochs takes 9.00299715996 seconds\n",
      "epoc 3300: 8.60433\n",
      "100 epochs takes 9.25800800323 seconds\n",
      "epoc 3400: 8.41343\n",
      "100 epochs takes 9.14868998528 seconds\n",
      "epoc 3500: 8.23195\n",
      "100 epochs takes 9.17549395561 seconds\n",
      "epoc 3600: 8.05855\n",
      "100 epochs takes 8.9803211689 seconds\n",
      "epoc 3700: 7.89243\n",
      "100 epochs takes 8.9548740387 seconds\n",
      "epoc 3800: 7.73379\n",
      "100 epochs takes 8.80385994911 seconds\n",
      "epoc 3900: 7.58222\n",
      "100 epochs takes 8.8269507885 seconds\n",
      "epoc 4000: 7.43603\n",
      "100 epochs takes 8.99089717865 seconds\n",
      "epoc 4100: 7.29632\n",
      "100 epochs takes 9.08541297913 seconds\n",
      "epoc 4200: 7.16174\n",
      "100 epochs takes 9.28933691978 seconds\n",
      "epoc 4300: 7.03268\n",
      "100 epochs takes 8.96507096291 seconds\n",
      "epoc 4400: 6.909\n",
      "100 epochs takes 9.39710307121 seconds\n",
      "epoc 4500: 6.78875\n",
      "100 epochs takes 8.90583276749 seconds\n",
      "epoc 4600: 6.67336\n",
      "100 epochs takes 9.05141997337 seconds\n",
      "epoc 4700: 6.56218\n",
      "100 epochs takes 8.9608528614 seconds\n",
      "epoc 4800: 6.45487\n",
      "100 epochs takes 8.85131001472 seconds\n",
      "epoc 4900: 6.35104\n",
      "100 epochs takes 9.39655399323 seconds\n",
      "epoc 5000: 6.25144\n",
      "100 epochs takes 8.91017007828 seconds\n",
      "epoc 5100: 6.15445\n",
      "100 epochs takes 8.99461984634 seconds\n",
      "epoc 5200: 6.06074\n",
      "100 epochs takes 8.90814495087 seconds\n",
      "epoc 5300: 5.97022\n",
      "100 epochs takes 8.97487306595 seconds\n",
      "epoc 5400: 5.88268\n",
      "100 epochs takes 8.99152302742 seconds\n",
      "epoc 5500: 5.79765\n",
      "100 epochs takes 9.04438400269 seconds\n",
      "epoc 5600: 5.7155\n",
      "100 epochs takes 8.75118112564 seconds\n",
      "epoc 5700: 5.63531\n",
      "100 epochs takes 8.78787493706 seconds\n",
      "epoc 5800: 5.55775\n",
      "100 epochs takes 8.87168598175 seconds\n",
      "epoc 5900: 5.48237\n",
      "100 epochs takes 8.75983810425 seconds\n",
      "epoc 6000: 5.40955\n",
      "100 epochs takes 8.96203184128 seconds\n",
      "epoc 6100: 5.33841\n",
      "100 epochs takes 8.83202695847 seconds\n",
      "epoc 6200: 5.26926\n",
      "100 epochs takes 9.0251698494 seconds\n",
      "epoc 6300: 5.20243\n",
      "100 epochs takes 8.99802994728 seconds\n",
      "epoc 6400: 5.13684\n",
      "100 epochs takes 9.07689094543 seconds\n",
      "epoc 6500: 5.07333\n",
      "100 epochs takes 9.12474298477 seconds\n",
      "epoc 6600: 5.0115\n",
      "100 epochs takes 8.97813200951 seconds\n",
      "epoc 6700: 4.95084\n",
      "100 epochs takes 8.87551784515 seconds\n",
      "epoc 6800: 4.89225\n",
      "100 epochs takes 8.84768891335 seconds\n",
      "epoc 6900: 4.83498\n",
      "100 epochs takes 8.87118601799 seconds\n",
      "epoc 7000: 4.77906\n",
      "100 epochs takes 8.90713715553 seconds\n",
      "epoc 7100: 4.72478\n",
      "100 epochs takes 8.8008480072 seconds\n",
      "epoc 7200: 4.67155\n",
      "100 epochs takes 9.08423781395 seconds\n",
      "epoc 7300: 4.61976\n",
      "100 epochs takes 9.16846489906 seconds\n",
      "epoc 7400: 4.56901\n",
      "100 epochs takes 8.78410005569 seconds\n",
      "epoc 7500: 4.51946\n",
      "100 epochs takes 9.10500884056 seconds\n",
      "epoc 7600: 4.47121\n",
      "100 epochs takes 9.28625392914 seconds\n",
      "epoc 7700: 4.42387\n",
      "100 epochs takes 9.01584601402 seconds\n",
      "epoc 7800: 4.37808\n",
      "100 epochs takes 9.02753996849 seconds\n",
      "epoc 7900: 4.33277\n",
      "100 epochs takes 8.8073720932 seconds\n",
      "epoc 8000: 4.28853\n",
      "100 epochs takes 9.11186099052 seconds\n",
      "epoc 8100: 4.24542\n",
      "100 epochs takes 8.84149217606 seconds\n",
      "epoc 8200: 4.20317\n",
      "100 epochs takes 9.14153385162 seconds\n",
      "epoc 8300: 4.16191\n",
      "100 epochs takes 9.28389811516 seconds\n",
      "epoc 8400: 4.12135\n",
      "100 epochs takes 9.21483898163 seconds\n",
      "epoc 8500: 4.08176\n",
      "100 epochs takes 9.01534509659 seconds\n",
      "epoc 8600: 4.04291\n",
      "100 epochs takes 9.03191614151 seconds\n",
      "epoc 8700: 4.00554\n",
      "100 epochs takes 8.86402392387 seconds\n",
      "epoc 8800: 3.96768\n",
      "100 epochs takes 9.02093601227 seconds\n",
      "epoc 8900: 3.93112\n",
      "100 epochs takes 9.21815180779 seconds\n",
      "epoc 9000: 3.8953\n",
      "100 epochs takes 9.00927686691 seconds\n",
      "epoc 9100: 3.86031\n",
      "100 epochs takes 8.97904706001 seconds\n",
      "epoc 9200: 3.82586\n",
      "100 epochs takes 8.95766806602 seconds\n",
      "epoc 9300: 3.7921\n",
      "100 epochs takes 9.04226517677 seconds\n",
      "epoc 9400: 3.7591\n",
      "100 epochs takes 9.04830908775 seconds\n",
      "epoc 9500: 3.72662\n",
      "100 epochs takes 8.87337398529 seconds\n",
      "epoc 9600: 3.69463\n",
      "100 epochs takes 9.24768996239 seconds\n",
      "epoc 9700: 3.66333\n",
      "100 epochs takes 9.24598193169 seconds\n",
      "epoc 9800: 3.63265\n",
      "100 epochs takes 8.82478404045 seconds\n",
      "epoc 9900: 3.6024\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid10=train(data10, 10, 10000, internal_act='tanh', learning_rate=0.1, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 100\n",
      "100 epochs takes 11.51684618 seconds\n",
      "epoc 0: 9111.85\n",
      "100 epochs takes 11.5236680508 seconds\n",
      "epoc 100: 2590.8\n",
      "100 epochs takes 11.5863921642 seconds\n",
      "epoc 200: 1321.07\n",
      "100 epochs takes 11.642277956 seconds\n",
      "epoc 300: 901.515\n",
      "100 epochs takes 11.6188299656 seconds\n",
      "epoc 400: 730.788\n",
      "100 epochs takes 11.588654995 seconds\n",
      "epoc 500: 637.645\n",
      "100 epochs takes 11.7546470165 seconds\n",
      "epoc 600: 576.195\n",
      "100 epochs takes 11.7699801922 seconds\n",
      "epoc 700: 528.786\n",
      "100 epochs takes 11.7428228855 seconds\n",
      "epoc 800: 484.707\n",
      "100 epochs takes 12.0463659763 seconds\n",
      "epoc 900: 430.7\n",
      "100 epochs takes 11.7655608654 seconds\n",
      "epoc 1000: 365.675\n",
      "100 epochs takes 11.9206221104 seconds\n",
      "epoc 1100: 311.159\n",
      "100 epochs takes 11.8484351635 seconds\n",
      "epoc 1200: 272.315\n",
      "100 epochs takes 11.7809638977 seconds\n",
      "epoc 1300: 245.158\n",
      "100 epochs takes 11.9608340263 seconds\n",
      "epoc 1400: 225.124\n",
      "100 epochs takes 11.8433690071 seconds\n",
      "epoc 1500: 209.313\n",
      "100 epochs takes 11.9539070129 seconds\n",
      "epoc 1600: 195.996\n",
      "100 epochs takes 11.8835258484 seconds\n",
      "epoc 1700: 184.137\n",
      "100 epochs takes 11.8467459679 seconds\n",
      "epoc 1800: 173.077\n",
      "100 epochs takes 11.715572834 seconds\n",
      "epoc 1900: 162.413\n",
      "100 epochs takes 11.7422690392 seconds\n",
      "epoc 2000: 151.917\n",
      "100 epochs takes 11.7390151024 seconds\n",
      "epoc 2100: 141.512\n",
      "100 epochs takes 11.7286472321 seconds\n",
      "epoc 2200: 131.242\n",
      "100 epochs takes 11.6341650486 seconds\n",
      "epoc 2300: 121.221\n",
      "100 epochs takes 11.7403988838 seconds\n",
      "epoc 2400: 111.652\n",
      "100 epochs takes 11.691838026 seconds\n",
      "epoc 2500: 102.802\n",
      "100 epochs takes 11.6715891361 seconds\n",
      "epoc 2600: 94.8775\n",
      "100 epochs takes 12.1928348541 seconds\n",
      "epoc 2700: 87.9295\n",
      "100 epochs takes 11.7677659988 seconds\n",
      "epoc 2800: 81.8828\n",
      "100 epochs takes 11.8544280529 seconds\n",
      "epoc 2900: 76.6225\n",
      "100 epochs takes 11.8493819237 seconds\n",
      "epoc 3000: 72.0271\n",
      "100 epochs takes 11.8296599388 seconds\n",
      "epoc 3100: 67.9852\n",
      "100 epochs takes 11.7292470932 seconds\n",
      "epoc 3200: 64.413\n",
      "100 epochs takes 11.6891329288 seconds\n",
      "epoc 3300: 61.2351\n",
      "100 epochs takes 11.8531649113 seconds\n",
      "epoc 3400: 58.3886\n",
      "100 epochs takes 12.0546710491 seconds\n",
      "epoc 3500: 55.8275\n",
      "100 epochs takes 11.9626011848 seconds\n",
      "epoc 3600: 53.5081\n",
      "100 epochs takes 11.9730899334 seconds\n",
      "epoc 3700: 51.3998\n",
      "100 epochs takes 11.9464480877 seconds\n",
      "epoc 3800: 49.4727\n",
      "100 epochs takes 11.8331229687 seconds\n",
      "epoc 3900: 47.7046\n",
      "100 epochs takes 11.8558030128 seconds\n",
      "epoc 4000: 46.0749\n",
      "100 epochs takes 11.9944179058 seconds\n",
      "epoc 4100: 44.5679\n",
      "100 epochs takes 11.9929759502 seconds\n",
      "epoc 4200: 43.1699\n",
      "100 epochs takes 12.0159552097 seconds\n",
      "epoc 4300: 41.8688\n",
      "100 epochs takes 11.991920948 seconds\n",
      "epoc 4400: 40.654\n",
      "100 epochs takes 11.9732170105 seconds\n",
      "epoc 4500: 39.5169\n",
      "100 epochs takes 11.9317910671 seconds\n",
      "epoc 4600: 38.4497\n",
      "100 epochs takes 12.0005638599 seconds\n",
      "epoc 4700: 37.445\n",
      "100 epochs takes 11.9755480289 seconds\n",
      "epoc 4800: 36.4985\n",
      "100 epochs takes 11.9955661297 seconds\n",
      "epoc 4900: 35.6032\n",
      "100 epochs takes 12.0089600086 seconds\n",
      "epoc 5000: 34.7567\n",
      "100 epochs takes 12.0615420341 seconds\n",
      "epoc 5100: 33.9536\n",
      "100 epochs takes 11.8873550892 seconds\n",
      "epoc 5200: 33.191\n",
      "100 epochs takes 11.8203430176 seconds\n",
      "epoc 5300: 32.4653\n",
      "100 epochs takes 11.9144730568 seconds\n",
      "epoc 5400: 31.7735\n",
      "100 epochs takes 11.9335768223 seconds\n",
      "epoc 5500: 31.1138\n",
      "100 epochs takes 11.8567769527 seconds\n",
      "epoc 5600: 30.4833\n",
      "100 epochs takes 12.0305149555 seconds\n",
      "epoc 5700: 29.8802\n",
      "100 epochs takes 12.0419919491 seconds\n",
      "epoc 5800: 29.3019\n",
      "100 epochs takes 12.0018701553 seconds\n",
      "epoc 5900: 28.7474\n",
      "100 epochs takes 12.0580499172 seconds\n",
      "epoc 6000: 28.2157\n",
      "100 epochs takes 12.1110620499 seconds\n",
      "epoc 6100: 27.7048\n",
      "100 epochs takes 11.9219121933 seconds\n",
      "epoc 6200: 27.213\n",
      "100 epochs takes 11.9234969616 seconds\n",
      "epoc 6300: 26.7398\n",
      "100 epochs takes 11.7684190273 seconds\n",
      "epoc 6400: 26.2839\n",
      "100 epochs takes 11.9424631596 seconds\n",
      "epoc 6500: 25.8441\n",
      "100 epochs takes 11.6554481983 seconds\n",
      "epoc 6600: 25.4198\n",
      "100 epochs takes 11.6210398674 seconds\n",
      "epoc 6700: 25.0099\n",
      "100 epochs takes 11.7195339203 seconds\n",
      "epoc 6800: 24.6139\n",
      "100 epochs takes 11.7544260025 seconds\n",
      "epoc 6900: 24.2312\n",
      "100 epochs takes 11.7879700661 seconds\n",
      "epoc 7000: 23.8601\n",
      "100 epochs takes 11.8099918365 seconds\n",
      "epoc 7100: 23.5008\n",
      "100 epochs takes 11.734937191 seconds\n",
      "epoc 7200: 23.1525\n",
      "100 epochs takes 11.6930429935 seconds\n",
      "epoc 7300: 22.815\n",
      "100 epochs takes 11.6788179874 seconds\n",
      "epoc 7400: 22.4872\n",
      "100 epochs takes 11.854170084 seconds\n",
      "epoc 7500: 22.1692\n",
      "100 epochs takes 11.8347308636 seconds\n",
      "epoc 7600: 21.8598\n",
      "100 epochs takes 11.8830230236 seconds\n",
      "epoc 7700: 21.5594\n",
      "100 epochs takes 11.8762848377 seconds\n",
      "epoc 7800: 21.2676\n",
      "100 epochs takes 11.7575900555 seconds\n",
      "epoc 7900: 20.9847\n",
      "100 epochs takes 11.6388039589 seconds\n",
      "epoc 8000: 20.7092\n",
      "100 epochs takes 11.7657740116 seconds\n",
      "epoc 8100: 20.4409\n",
      "100 epochs takes 11.7834579945 seconds\n",
      "epoc 8200: 20.1796\n",
      "100 epochs takes 11.8353819847 seconds\n",
      "epoc 8300: 19.9252\n",
      "100 epochs takes 11.8330609798 seconds\n",
      "epoc 8400: 19.6773\n",
      "100 epochs takes 11.9323680401 seconds\n",
      "epoc 8500: 19.4349\n",
      "100 epochs takes 11.8077509403 seconds\n",
      "epoc 8600: 19.1994\n",
      "100 epochs takes 11.8920209408 seconds\n",
      "epoc 8700: 18.9687\n",
      "100 epochs takes 11.7785620689 seconds\n",
      "epoc 8800: 18.7443\n",
      "100 epochs takes 11.7298018932 seconds\n",
      "epoc 8900: 18.5244\n",
      "100 epochs takes 11.7377970219 seconds\n",
      "epoc 9000: 18.3099\n",
      "100 epochs takes 11.8393571377 seconds\n",
      "epoc 9100: 18.0999\n",
      "100 epochs takes 11.7830262184 seconds\n",
      "epoc 9200: 17.8946\n",
      "100 epochs takes 11.758202076 seconds\n",
      "epoc 9300: 17.6943\n",
      "100 epochs takes 11.8239820004 seconds\n",
      "epoc 9400: 17.4976\n",
      "100 epochs takes 11.7874960899 seconds\n",
      "epoc 9500: 17.3061\n",
      "100 epochs takes 11.7503261566 seconds\n",
      "epoc 9600: 17.119\n",
      "100 epochs takes 11.8413128853 seconds\n",
      "epoc 9700: 16.936\n",
      "100 epochs takes 11.8964400291 seconds\n",
      "epoc 9800: 16.7567\n",
      "100 epochs takes 11.8337278366 seconds\n",
      "epoc 9900: 16.5815\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid10=train(data10, 10, 10000, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 30 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1256.  1377.  1467.  1251.  1521.  1229.  2490.  1695.  1291.  1387.\n",
      "  1275.  1743.  1443.  1497.  3383.  2484.  2758.  1666.  1255.  2476.\n",
      "  1789.  1358.  1269.  1395.  2133.  3011.  3239.  1568.  2155.  1509.]\n",
      "<NDArray 30 @cpu(0)>\n",
      "(81306L, 30L)\n"
     ]
    }
   ],
   "source": [
    "sp_data30 = get_densest(spm, 30)\n",
    "data30 = mx.ndarray.sparse.csr_matrix(sp_data30)\n",
    "print(mx.ndarray.sum(data30, axis=0))\n",
    "print(data30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd error: 12539.0\n"
     ]
    }
   ],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data30, k=15)\n",
    "res = np.dot(sp_data30.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 5000\n",
      "100 epochs takes 20.6709759235 seconds\n",
      "epoc 100: 48314.7\n",
      "100 epochs takes 20.4837198257 seconds\n",
      "epoc 200: 38406.1\n",
      "100 epochs takes 20.5601148605 seconds\n",
      "epoc 300: 34322.2\n",
      "100 epochs takes 20.5915081501 seconds\n",
      "epoc 400: 30803.0\n",
      "100 epochs takes 20.6268348694 seconds\n",
      "epoc 500: 27163.7\n",
      "100 epochs takes 20.582613945 seconds\n",
      "epoc 600: 24936.0\n",
      "100 epochs takes 20.5398068428 seconds\n",
      "epoc 700: 23276.2\n",
      "100 epochs takes 20.6115128994 seconds\n",
      "epoc 800: 21595.4\n",
      "100 epochs takes 20.6879820824 seconds\n",
      "epoc 900: 19896.2\n",
      "100 epochs takes 20.4795188904 seconds\n",
      "epoc 1000: 18392.6\n",
      "100 epochs takes 20.4407179356 seconds\n",
      "epoc 1100: 16991.6\n",
      "100 epochs takes 20.6663501263 seconds\n",
      "epoc 1200: 15279.4\n",
      "100 epochs takes 20.5412111282 seconds\n",
      "epoc 1300: 13563.2\n",
      "100 epochs takes 20.3222379684 seconds\n",
      "epoc 1400: 12011.7\n",
      "100 epochs takes 20.4241900444 seconds\n",
      "epoc 1500: 10697.8\n",
      "100 epochs takes 20.3226759434 seconds\n",
      "epoc 1600: 9618.12\n",
      "100 epochs takes 20.3310542107 seconds\n",
      "epoc 1700: 8649.68\n",
      "100 epochs takes 20.6438128948 seconds\n",
      "epoc 1800: 7655.73\n",
      "100 epochs takes 20.7151269913 seconds\n",
      "epoc 1900: 6797.63\n",
      "100 epochs takes 20.8625690937 seconds\n",
      "epoc 2000: 6035.9\n",
      "100 epochs takes 20.7823748589 seconds\n",
      "epoc 2100: 5444.06\n",
      "100 epochs takes 20.8244009018 seconds\n",
      "epoc 2200: 5048.34\n",
      "100 epochs takes 20.6987831593 seconds\n",
      "epoc 2300: 4763.53\n",
      "100 epochs takes 20.7514140606 seconds\n",
      "epoc 2400: 4543.0\n",
      "100 epochs takes 20.6554589272 seconds\n",
      "epoc 2500: 4363.0\n",
      "100 epochs takes 20.5867118835 seconds\n",
      "epoc 2600: 4210.46\n",
      "100 epochs takes 20.5393970013 seconds\n",
      "epoc 2700: 4077.63\n",
      "100 epochs takes 20.3896760941 seconds\n",
      "epoc 2800: 3959.57\n",
      "100 epochs takes 20.5033910275 seconds\n",
      "epoc 2900: 3853.02\n",
      "100 epochs takes 20.3370718956 seconds\n",
      "epoc 3000: 3755.62\n",
      "100 epochs takes 20.5290808678 seconds\n",
      "epoc 3100: 3665.65\n",
      "100 epochs takes 20.5946631432 seconds\n",
      "epoc 3200: 3581.76\n",
      "100 epochs takes 20.4921309948 seconds\n",
      "epoc 3300: 3502.85\n",
      "100 epochs takes 20.430423975 seconds\n",
      "epoc 3400: 3428.05\n",
      "100 epochs takes 20.5521929264 seconds\n",
      "epoc 3500: 3356.63\n",
      "100 epochs takes 20.494603157 seconds\n",
      "epoc 3600: 3288.2\n",
      "100 epochs takes 20.477930069 seconds\n",
      "epoc 3700: 3222.68\n",
      "100 epochs takes 21.1403241158 seconds\n",
      "epoc 3800: 3160.22\n",
      "100 epochs takes 20.4911570549 seconds\n",
      "epoc 3900: 3100.8\n",
      "100 epochs takes 20.2122118473 seconds\n",
      "epoc 4000: 3044.17\n",
      "100 epochs takes 20.5016150475 seconds\n",
      "epoc 4100: 2990.05\n",
      "100 epochs takes 20.5713689327 seconds\n",
      "epoc 4200: 2938.15\n",
      "100 epochs takes 20.5105280876 seconds\n",
      "epoc 4300: 2888.24\n",
      "100 epochs takes 20.3579649925 seconds\n",
      "epoc 4400: 2840.16\n",
      "100 epochs takes 20.293954134 seconds\n",
      "epoc 4500: 2793.84\n",
      "100 epochs takes 20.4679169655 seconds\n",
      "epoc 4600: 2749.3\n",
      "100 epochs takes 20.6458499432 seconds\n",
      "epoc 4700: 2706.57\n",
      "100 epochs takes 20.6692960262 seconds\n",
      "epoc 4800: 2665.71\n",
      "100 epochs takes 20.7215337753 seconds\n",
      "epoc 4900: 2626.72\n",
      "100 epochs takes 20.6110918522 seconds\n",
      "epoc 5000: 2589.55\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid30=train(data30, num_dims=15, num_epoc=5000, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 100 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1256.   923.  1180.   802.   979.   744.   948.   837.  1377.  1183.\n",
      "  1023.  1467.   748.  1251.  1198.   921.   818.   777.   889.  1185.\n",
      "  1521.   744.  1229.   916.   810.   804.   844.  1051.   861.   991.\n",
      "   784.  1225.  2490.   975.   915.   996.  1030.   822.   806.   756.\n",
      "  1695.   780.  1053.   964.  1291.  1103.  1091.  1387.  1275.   889.\n",
      "   828.  1743.  1443.   959.  1497.  3383.   798.   756.  1080.   937.\n",
      "   758.  2484.   879.   849.   766.   737.  2758.   733.   788.   878.\n",
      "  1666.   857.  1079.   902.   783.  1255.  2476.   780.  1789.  1054.\n",
      "   932.  1358.  1269.  1161.   765.   912.   818.  1220.  1395.  1009.\n",
      "   801.  2133.  3011.  3239.  1568.  2155.  1072.  1509.  1226.   779.]\n",
      "<NDArray 100 @cpu(0)>\n",
      "(81306L, 100L)\n"
     ]
    }
   ],
   "source": [
    "sp_data100 = get_densest(spm, 100)\n",
    "data100 = mx.ndarray.sparse.csr_matrix(sp_data100)\n",
    "print(mx.ndarray.sum(data100, axis=0))\n",
    "print(data100.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to start with SVD and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd error: 42371.5\n"
     ]
    }
   ],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data100, k=30)\n",
    "res = np.dot(sp_data100.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With curriculum learning, we can take advantage of the computation results from the denser columns.\n",
    "To some extent, curriculum learning provides a new way of initializing the parameters of a neural network. We take the parameters from the neural network trained for the denser columns of a graph to initialize the neural network for the dataset with more columns.\n",
    "\n",
    "The functions below expand and shrink the weight matrices for the smaller or the larger neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the weight matrices in the encoder of the smaller autoencoder to\n",
    "# the shape required by the larger autoencoder.\n",
    "def extend_params_encode(weight, bias, idx, num_inputs, num_outputs, rand_init):\n",
    "    if (rand_init):\n",
    "        # We initialize the weights in the same way as MXNet\n",
    "        out_weight = np.random.uniform(low=-0.1, high=0.1, size=(num_outputs, num_inputs))\n",
    "    else:\n",
    "        out_weight = np.zeros((num_outputs, num_inputs))\n",
    "    out_bias = np.zeros(num_outputs)\n",
    "    out_weight[0:weight.shape[0], idx] = weight.asnumpy()\n",
    "    out_bias[0:bias.shape[0]] = bias.asnumpy()\n",
    "    return mx.nd.array(out_weight), mx.nd.array(out_bias)\n",
    "\n",
    "# Extend the weight matrices in the decoder of the smaller autoencoder to\n",
    "# the shape required by the larger autoencoder.\n",
    "def extend_params_decode(weight, bias, idx, num_inputs, num_outputs, rand_init):\n",
    "    if (rand_init):\n",
    "        # We initialize the weights in the same way as MXNet\n",
    "        out_weight = np.random.uniform(low=-0.1, high=0.1, size=(num_outputs, num_inputs))\n",
    "    else:\n",
    "        out_weight = np.zeros((num_outputs, num_inputs))\n",
    "    out_bias = np.zeros(num_outputs)\n",
    "    out_weight[idx, 0:weight.shape[1]] = weight.asnumpy()\n",
    "    out_bias[idx] = bias.asnumpy()\n",
    "    return mx.nd.array(out_weight), mx.nd.array(out_bias)\n",
    "\n",
    "# Shrink the weight matrices in the encoder of the larger autoencoder to\n",
    "# the shape required by the smaller autoencoder. This operation is\n",
    "# the reverse of extend_params_encode.\n",
    "def shrink_params_encode(weight, bias, idx, num_inputs, num_outputs):\n",
    "    weight = weight.asnumpy()\n",
    "    bias = bias.asnumpy()\n",
    "    return mx.nd.array(weight[0:num_outputs, idx]), mx.nd.array(bias[0:num_outputs])\n",
    "\n",
    "# Shrink the weight matrices in the decoder of the larger autoencoder to\n",
    "# the shape required by the smaller autoencoder. This operation is\n",
    "# the reverse of extend_params_decode.\n",
    "def shrink_params_decode(weight, bias, idx, num_inputs, num_outputs):\n",
    "    weight = weight.asnumpy()\n",
    "    bias = bias.asnumpy()\n",
    "    return mx.nd.array(weight[idx, 0:num_inputs]), mx.nd.array(bias[idx])\n",
    "\n",
    "# This function extends the parameter matrices in the small autoencoder\n",
    "# to an autoencoder with the specified number of input nodes and hidden nodes.\n",
    "def extend_params(params, new_data, new_hidden, rand_init=False):\n",
    "    old_inputs = params.get(weight_names[0]).shape[1]\n",
    "    old_hidden = params.get(bias_names[0]).shape[0]\n",
    "    new_inputs = new_data.shape[1]\n",
    "    max_cols, max_idx = get_densest_idx(new_data, old_inputs)\n",
    "    weight, bias = extend_params_encode(params.get(weight_names[0]),\n",
    "                                        params.get(bias_names[0]), max_idx,\n",
    "                                        new_inputs, new_hidden, rand_init)\n",
    "    new_params = {}\n",
    "    new_params.update({weight_names[0]: weight})\n",
    "    new_params.update({bias_names[0]: bias})\n",
    "    weight, bias = extend_params_decode(params.get(weight_names[1]),\n",
    "                                        params.get(bias_names[1]), max_idx,\n",
    "                                        new_hidden, new_inputs, rand_init)\n",
    "    new_params.update({weight_names[1]: weight})\n",
    "    new_params.update({bias_names[1]: bias})\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section is to verify the implementation of the functions above work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old inputs: 30\n",
      "old hidden: 15\n",
      "new inputs: 100\n",
      "2589.55\n",
      "2589.55\n"
     ]
    }
   ],
   "source": [
    "def cal_model_numpy(data, params, selected_cols=None, internal_act=None, output_act=None):\n",
    "    fc1_weight = params.get(weight_names[0]).asnumpy()\n",
    "    fc1_bias = params.get(bias_names[0]).asnumpy()\n",
    "    fc2_weight = params.get(weight_names[1]).asnumpy()\n",
    "    fc2_bias = params.get(bias_names[1]).asnumpy()\n",
    "\n",
    "    np_data = data.asnumpy()\n",
    "    hidden = np.dot(np_data, fc1_weight.T) + fc1_bias\n",
    "    act_func = get_act(internal_act)\n",
    "    if (act_func is not None):\n",
    "        hidden = act_func(hidden)\n",
    "    output = np.dot(hidden, fc2_weight.T) + fc2_bias\n",
    "    act_func = get_act(output_act)\n",
    "    if (act_func is not None):\n",
    "        output = act_func(output)\n",
    "    if (selected_cols is None):\n",
    "        return np.sum(np.square(output - np_data))\n",
    "    else:\n",
    "        return np.sum(np.square(output[:,selected_cols] - np_data[:,selected_cols]))\n",
    "\n",
    "tmp30 = params_sigmoid30\n",
    "tmp100 = extend_params(tmp30, sp_data100, 30, rand_init=False)\n",
    "max_cols30, max_idx30 = get_densest_idx(sp_data100, 30)\n",
    "print(cal_model_numpy(data30, tmp30, internal_act='tanh', output_act='sigmoid'))\n",
    "print(cal_model_numpy(data100, tmp100, selected_cols=max_idx30, internal_act='tanh', output_act='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the parameters from the smaller autoencoder to initialize the parameters in the larger autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 5000\n",
      "reinit optimizer. New learning rate: 0.2\n",
      "100 epochs takes 49.9413859844 seconds\n",
      "epoc 100: 65572.6\n",
      "100 epochs takes 50.3041770458 seconds\n",
      "epoc 200: 60734.8\n",
      "100 epochs takes 50.5001418591 seconds\n",
      "epoc 300: 58643.4\n",
      "100 epochs takes 50.1831459999 seconds\n",
      "epoc 400: 57395.7\n",
      "100 epochs takes 50.1922550201 seconds\n",
      "epoc 500: 56486.5\n",
      "100 epochs takes 50.0525450706 seconds\n",
      "epoc 600: 55708.6\n",
      "100 epochs takes 49.9641830921 seconds\n",
      "epoc 700: 54969.4\n",
      "100 epochs takes 50.0099928379 seconds\n",
      "epoc 800: 54234.4\n",
      "100 epochs takes 50.0637071133 seconds\n",
      "epoc 900: 53498.1\n",
      "100 epochs takes 50.0675020218 seconds\n",
      "epoc 1000: 52764.2\n",
      "100 epochs takes 49.951611042 seconds\n",
      "epoc 1100: 52038.1\n",
      "100 epochs takes 50.3725850582 seconds\n",
      "epoc 1200: 51327.1\n",
      "100 epochs takes 50.0172758102 seconds\n",
      "epoc 1300: 50640.2\n",
      "100 epochs takes 50.3492980003 seconds\n",
      "epoc 1400: 49986.9\n",
      "100 epochs takes 50.255428791 seconds\n",
      "epoc 1500: 49372.6\n",
      "100 epochs takes 49.867223978 seconds\n",
      "epoc 1600: 48797.8\n",
      "100 epochs takes 50.3340940475 seconds\n",
      "epoc 1700: 48257.8\n",
      "100 epochs takes 50.2100310326 seconds\n",
      "epoc 1800: 47746.1\n",
      "100 epochs takes 50.3251771927 seconds\n",
      "epoc 1900: 47255.0\n",
      "100 epochs takes 50.2721910477 seconds\n",
      "epoc 2000: 46777.8\n",
      "100 epochs takes 50.1794080734 seconds\n",
      "epoc 2100: 46308.0\n",
      "100 epochs takes 50.203081131 seconds\n",
      "epoc 2200: 45840.3\n",
      "100 epochs takes 50.1995530128 seconds\n",
      "epoc 2300: 45370.9\n",
      "100 epochs takes 49.8047161102 seconds\n",
      "epoc 2400: 44896.8\n",
      "100 epochs takes 49.8718631268 seconds\n",
      "epoc 2500: 44416.8\n",
      "100 epochs takes 49.7709000111 seconds\n",
      "epoc 2600: 43930.7\n",
      "100 epochs takes 49.7330870628 seconds\n",
      "epoc 2700: 43438.7\n",
      "100 epochs takes 49.7025458813 seconds\n",
      "epoc 2800: 42940.9\n",
      "100 epochs takes 49.6586050987 seconds\n",
      "epoc 2900: 42436.6\n",
      "100 epochs takes 49.5676500797 seconds\n",
      "epoc 3000: 41924.1\n",
      "100 epochs takes 50.011526823 seconds\n",
      "epoc 3100: 41400.6\n",
      "100 epochs takes 49.9558308125 seconds\n",
      "epoc 3200: 40862.6\n",
      "100 epochs takes 50.0549240112 seconds\n",
      "epoc 3300: 40305.6\n",
      "100 epochs takes 50.026403904 seconds\n",
      "epoc 3400: 39725.0\n",
      "100 epochs takes 49.9101848602 seconds\n",
      "epoc 3500: 39117.5\n",
      "100 epochs takes 49.6173648834 seconds\n",
      "epoc 3600: 38483.2\n",
      "100 epochs takes 49.6396389008 seconds\n",
      "epoc 3700: 37826.8\n",
      "100 epochs takes 49.6907877922 seconds\n",
      "epoc 3800: 37154.6\n",
      "100 epochs takes 49.705436945 seconds\n",
      "epoc 3900: 36473.1\n",
      "100 epochs takes 49.5786118507 seconds\n",
      "epoc 4000: 35789.4\n",
      "100 epochs takes 49.7333629131 seconds\n",
      "epoc 4100: 35110.2\n",
      "100 epochs takes 49.5664792061 seconds\n",
      "epoc 4200: 34437.9\n",
      "100 epochs takes 49.5697610378 seconds\n",
      "epoc 4300: 33772.0\n",
      "100 epochs takes 49.5752191544 seconds\n",
      "epoc 4400: 33114.2\n",
      "100 epochs takes 49.4479448795 seconds\n",
      "epoc 4500: 32471.0\n",
      "100 epochs takes 49.5633449554 seconds\n",
      "epoc 4600: 31849.3\n",
      "100 epochs takes 50.0002570152 seconds\n",
      "epoc 4700: 31252.3\n",
      "100 epochs takes 49.4906420708 seconds\n",
      "epoc 4800: 30680.8\n",
      "100 epochs takes 49.8560187817 seconds\n",
      "epoc 4900: 30134.3\n",
      "100 epochs takes 49.7987689972 seconds\n",
      "epoc 5000: 29611.6\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid100_init = extend_params(params_sigmoid30, sp_data100, 30, rand_init=True)\n",
    "model = AutoEncoderModel(data100, 30, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)\n",
    "params_sigmoid100 = model.train(data100, num_epoc=5000, params=params_sigmoid100_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal #epochs: 5000\n",
      "reinit optimizer. New learning rate: 0.2\n",
      "100 epochs takes 49.8752069473 seconds\n",
      "epoc 100: 29111.8\n",
      "100 epochs takes 49.6204049587 seconds\n",
      "epoc 200: 28630.2\n",
      "100 epochs takes 49.497205019 seconds\n",
      "epoc 300: 28165.3\n",
      "100 epochs takes 49.4390890598 seconds\n",
      "epoc 400: 27714.1\n",
      "100 epochs takes 49.4647581577 seconds\n",
      "epoc 500: 27273.9\n",
      "100 epochs takes 49.4208300114 seconds\n",
      "epoc 600: 26841.9\n",
      "100 epochs takes 49.414003849 seconds\n",
      "epoc 700: 26415.7\n",
      "100 epochs takes 49.3721160889 seconds\n",
      "epoc 800: 25993.1\n",
      "100 epochs takes 49.5414528847 seconds\n",
      "epoc 900: 25572.0\n",
      "100 epochs takes 49.5266869068 seconds\n",
      "epoc 1000: 25151.0\n",
      "100 epochs takes 49.6930439472 seconds\n",
      "epoc 1100: 24729.8\n",
      "100 epochs takes 49.8241529465 seconds\n",
      "epoc 1200: 24309.3\n",
      "100 epochs takes 49.9031429291 seconds\n",
      "epoc 1300: 23892.1\n",
      "100 epochs takes 50.006289959 seconds\n",
      "epoc 1400: 23481.1\n",
      "100 epochs takes 49.9422609806 seconds\n",
      "epoc 1500: 23078.3\n",
      "100 epochs takes 49.8204278946 seconds\n",
      "epoc 1600: 22684.4\n",
      "100 epochs takes 49.971832037 seconds\n",
      "epoc 1700: 22299.4\n",
      "100 epochs takes 49.9025950432 seconds\n",
      "epoc 1800: 21922.7\n",
      "100 epochs takes 50.00187397 seconds\n",
      "epoc 1900: 21553.6\n",
      "100 epochs takes 49.886548996 seconds\n",
      "epoc 2000: 21191.5\n",
      "100 epochs takes 49.3841340542 seconds\n",
      "epoc 2100: 20836.1\n",
      "100 epochs takes 49.5099020004 seconds\n",
      "epoc 2200: 20486.9\n",
      "100 epochs takes 49.3345971107 seconds\n",
      "epoc 2300: 20143.9\n",
      "100 epochs takes 49.5953299999 seconds\n",
      "epoc 2400: 19807.7\n",
      "100 epochs takes 49.4452140331 seconds\n",
      "epoc 2500: 19479.5\n",
      "100 epochs takes 49.4766931534 seconds\n",
      "epoc 2600: 19160.7\n",
      "100 epochs takes 49.5074458122 seconds\n",
      "epoc 2700: 18852.5\n",
      "100 epochs takes 49.4515140057 seconds\n",
      "epoc 2800: 18555.6\n",
      "100 epochs takes 49.4640440941 seconds\n",
      "epoc 2900: 18270.6\n",
      "100 epochs takes 49.5378220081 seconds\n",
      "epoc 3000: 17997.6\n",
      "100 epochs takes 49.7498190403 seconds\n",
      "epoc 3100: 17736.4\n",
      "100 epochs takes 49.4500648975 seconds\n",
      "epoc 3200: 17487.0\n",
      "100 epochs takes 49.887139082 seconds\n",
      "epoc 3300: 17248.8\n",
      "100 epochs takes 50.0054759979 seconds\n",
      "epoc 3400: 17021.4\n",
      "100 epochs takes 49.8517482281 seconds\n",
      "epoc 3500: 16804.3\n",
      "100 epochs takes 49.8643729687 seconds\n",
      "epoc 3600: 16596.8\n",
      "100 epochs takes 49.4415030479 seconds\n",
      "epoc 3700: 16398.5\n",
      "100 epochs takes 49.3618628979 seconds\n",
      "epoc 3800: 16208.6\n",
      "100 epochs takes 49.3800871372 seconds\n",
      "epoc 3900: 16026.6\n",
      "100 epochs takes 49.3072230816 seconds\n",
      "epoc 4000: 15851.9\n",
      "100 epochs takes 49.3655149937 seconds\n",
      "epoc 4100: 15683.9\n",
      "100 epochs takes 49.3760969639 seconds\n",
      "epoc 4200: 15522.0\n",
      "100 epochs takes 49.491686821 seconds\n",
      "epoc 4300: 15365.9\n",
      "100 epochs takes 49.3396661282 seconds\n",
      "epoc 4400: 15214.9\n",
      "100 epochs takes 49.3712348938 seconds\n",
      "epoc 4500: 15068.6\n",
      "100 epochs takes 49.4426670074 seconds\n",
      "epoc 4600: 14926.5\n",
      "100 epochs takes 49.3477940559 seconds\n",
      "epoc 4700: 14788.4\n",
      "100 epochs takes 49.4055809975 seconds\n",
      "epoc 4800: 14653.9\n",
      "100 epochs takes 49.6424980164 seconds\n",
      "epoc 4900: 14522.8\n",
      "100 epochs takes 49.3547041416 seconds\n",
      "epoc 5000: 14395.1\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid100_2 = model.train(data100, num_epoc=5000, params=params_sigmoid100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 10000\n",
      "100 epochs takes 50.0786571503 seconds\n",
      "epoc 100: 117194.0\n",
      "100 epochs takes 50.4721970558 seconds\n",
      "epoc 200: 115469.0\n",
      "100 epochs takes 50.4774711132 seconds\n",
      "epoc 300: 106096.0\n",
      "100 epochs takes 50.3924469948 seconds\n",
      "epoc 400: 96626.0\n",
      "100 epochs takes 50.4311208725 seconds\n",
      "epoc 500: 93401.9\n",
      "100 epochs takes 50.3713068962 seconds\n",
      "epoc 600: 91831.9\n",
      "100 epochs takes 50.5239970684 seconds\n",
      "epoc 700: 90197.9\n",
      "100 epochs takes 50.5635919571 seconds\n",
      "epoc 800: 88247.8\n",
      "100 epochs takes 50.560598135 seconds\n",
      "epoc 900: 86554.8\n",
      "100 epochs takes 50.1708669662 seconds\n",
      "epoc 1000: 85147.5\n",
      "100 epochs takes 50.513051033 seconds\n",
      "epoc 1100: 83771.5\n",
      "100 epochs takes 50.6233348846 seconds\n",
      "epoc 1200: 82242.6\n",
      "100 epochs takes 50.5886049271 seconds\n",
      "epoc 1300: 80421.5\n",
      "100 epochs takes 50.6030488014 seconds\n",
      "epoc 1400: 78234.4\n",
      "100 epochs takes 50.5480351448 seconds\n",
      "epoc 1500: 75972.2\n",
      "100 epochs takes 50.6316308975 seconds\n",
      "epoc 1600: 74030.9\n",
      "100 epochs takes 50.8175470829 seconds\n",
      "epoc 1700: 72408.9\n",
      "100 epochs takes 49.9566819668 seconds\n",
      "epoc 1800: 71014.1\n",
      "100 epochs takes 50.1811499596 seconds\n",
      "epoc 1900: 69782.1\n",
      "100 epochs takes 50.0322132111 seconds\n",
      "epoc 2000: 68659.2\n",
      "100 epochs takes 49.9976751804 seconds\n",
      "epoc 2100: 67599.4\n",
      "100 epochs takes 49.8896539211 seconds\n",
      "epoc 2200: 66566.9\n",
      "100 epochs takes 49.9521379471 seconds\n",
      "epoc 2300: 65536.9\n",
      "100 epochs takes 49.8326199055 seconds\n",
      "epoc 2400: 64496.4\n",
      "100 epochs takes 49.8126740456 seconds\n",
      "epoc 2500: 63443.1\n",
      "100 epochs takes 50.1703999043 seconds\n",
      "epoc 2600: 62383.8\n",
      "100 epochs takes 50.2458980083 seconds\n",
      "epoc 2700: 61329.4\n",
      "100 epochs takes 50.3421781063 seconds\n",
      "epoc 2800: 60290.0\n",
      "100 epochs takes 49.8610639572 seconds\n",
      "epoc 2900: 59269.9\n",
      "100 epochs takes 49.8092529774 seconds\n",
      "epoc 3000: 58266.9\n",
      "100 epochs takes 49.790019989 seconds\n",
      "epoc 3100: 57273.6\n",
      "100 epochs takes 49.7599279881 seconds\n",
      "epoc 3200: 56276.9\n",
      "100 epochs takes 49.8626980782 seconds\n",
      "epoc 3300: 55256.6\n",
      "100 epochs takes 49.8686699867 seconds\n",
      "epoc 3400: 54189.7\n",
      "100 epochs takes 49.7700619698 seconds\n",
      "epoc 3500: 53075.6\n",
      "100 epochs takes 50.1426019669 seconds\n",
      "epoc 3600: 51951.5\n",
      "100 epochs takes 50.2783529758 seconds\n",
      "epoc 3700: 50850.6\n",
      "100 epochs takes 50.2448430061 seconds\n",
      "epoc 3800: 49776.3\n",
      "100 epochs takes 49.7320320606 seconds\n",
      "epoc 3900: 48719.4\n",
      "100 epochs takes 49.8196361065 seconds\n",
      "epoc 4000: 47674.9\n",
      "100 epochs takes 49.7090950012 seconds\n",
      "epoc 4100: 46650.5\n",
      "100 epochs takes 49.9904639721 seconds\n",
      "epoc 4200: 45661.1\n",
      "100 epochs takes 50.1380131245 seconds\n",
      "epoc 4300: 44717.3\n",
      "100 epochs takes 50.1448040009 seconds\n",
      "epoc 4400: 43818.4\n",
      "100 epochs takes 50.106290102 seconds\n",
      "epoc 4500: 42955.5\n",
      "100 epochs takes 49.9703030586 seconds\n",
      "epoc 4600: 42118.1\n",
      "100 epochs takes 50.0013420582 seconds\n",
      "epoc 4700: 41299.5\n",
      "100 epochs takes 50.0716831684 seconds\n",
      "epoc 4800: 40498.1\n",
      "100 epochs takes 50.0076808929 seconds\n",
      "epoc 4900: 39714.3\n",
      "100 epochs takes 50.0509738922 seconds\n",
      "epoc 5000: 38947.9\n",
      "100 epochs takes 49.5324919224 seconds\n",
      "epoc 5100: 38198.4\n",
      "100 epochs takes 49.4852929115 seconds\n",
      "epoc 5200: 37466.4\n",
      "100 epochs takes 49.4799590111 seconds\n",
      "epoc 5300: 36751.6\n",
      "100 epochs takes 49.5578761101 seconds\n",
      "epoc 5400: 36055.1\n",
      "100 epochs takes 49.5502707958 seconds\n",
      "epoc 5500: 35382.6\n",
      "100 epochs takes 49.5495960712 seconds\n",
      "epoc 5600: 34743.5\n",
      "100 epochs takes 49.5515749454 seconds\n",
      "epoc 5700: 34143.4\n",
      "100 epochs takes 49.5338730812 seconds\n",
      "epoc 5800: 33582.0\n",
      "100 epochs takes 49.5047090054 seconds\n",
      "epoc 5900: 33055.1\n",
      "100 epochs takes 49.6062150002 seconds\n",
      "epoc 6000: 32557.5\n",
      "100 epochs takes 49.5076839924 seconds\n",
      "epoc 6100: 32083.8\n",
      "100 epochs takes 49.5636770725 seconds\n",
      "epoc 6200: 31629.6\n",
      "100 epochs takes 49.4636368752 seconds\n",
      "epoc 6300: 31190.6\n",
      "100 epochs takes 49.4432299137 seconds\n",
      "epoc 6400: 30763.6\n",
      "100 epochs takes 49.8680269718 seconds\n",
      "epoc 6500: 30345.5\n",
      "100 epochs takes 49.4586849213 seconds\n",
      "epoc 6600: 29934.1\n",
      "100 epochs takes 49.5235579014 seconds\n",
      "epoc 6700: 29527.5\n",
      "100 epochs takes 49.5194320679 seconds\n",
      "epoc 6800: 29124.3\n",
      "100 epochs takes 49.8689389229 seconds\n",
      "epoc 6900: 28723.9\n",
      "100 epochs takes 49.8806040287 seconds\n",
      "epoc 7000: 28325.9\n",
      "100 epochs takes 49.8382720947 seconds\n",
      "epoc 7100: 27930.3\n",
      "100 epochs takes 50.0399179459 seconds\n",
      "epoc 7200: 27537.2\n",
      "100 epochs takes 49.941947937 seconds\n",
      "epoc 7300: 27146.8\n",
      "100 epochs takes 49.427038908 seconds\n",
      "epoc 7400: 26759.6\n",
      "100 epochs takes 49.4447319508 seconds\n",
      "epoc 7500: 26376.1\n",
      "100 epochs takes 49.9383821487 seconds\n",
      "epoc 7600: 25996.5\n",
      "100 epochs takes 49.9351489544 seconds\n",
      "epoc 7700: 25621.6\n",
      "100 epochs takes 49.9292500019 seconds\n",
      "epoc 7800: 25251.1\n",
      "100 epochs takes 49.4475119114 seconds\n",
      "epoc 7900: 24885.2\n",
      "100 epochs takes 49.4211649895 seconds\n",
      "epoc 8000: 24523.4\n",
      "100 epochs takes 49.3887548447 seconds\n",
      "epoc 8100: 24165.3\n",
      "100 epochs takes 49.3054580688 seconds\n",
      "epoc 8200: 23810.6\n",
      "100 epochs takes 49.3268229961 seconds\n",
      "epoc 8300: 23459.5\n",
      "100 epochs takes 49.4158670902 seconds\n",
      "epoc 8400: 23113.2\n",
      "100 epochs takes 49.9363780022 seconds\n",
      "epoc 8500: 22773.6\n",
      "100 epochs takes 49.8816599846 seconds\n",
      "epoc 8600: 22443.3\n",
      "100 epochs takes 49.8217961788 seconds\n",
      "epoc 8700: 22123.8\n",
      "100 epochs takes 49.937828064 seconds\n",
      "epoc 8800: 21816.2\n",
      "100 epochs takes 49.3480198383 seconds\n",
      "epoc 8900: 21520.4\n",
      "100 epochs takes 49.5952470303 seconds\n",
      "epoc 9000: 21236.1\n",
      "100 epochs takes 49.9008080959 seconds\n",
      "epoc 9100: 20962.8\n",
      "100 epochs takes 49.8739919662 seconds\n",
      "epoc 9200: 20699.9\n",
      "100 epochs takes 49.4101178646 seconds\n",
      "epoc 9300: 20447.1\n",
      "100 epochs takes 49.3958001137 seconds\n",
      "epoc 9400: 20203.9\n",
      "100 epochs takes 49.7580749989 seconds\n",
      "epoc 9500: 19969.7\n",
      "100 epochs takes 49.3630189896 seconds\n",
      "epoc 9600: 19744.1\n",
      "100 epochs takes 49.3261668682 seconds\n",
      "epoc 9700: 19526.5\n",
      "100 epochs takes 49.4893898964 seconds\n",
      "epoc 9800: 19316.3\n",
      "100 epochs takes 49.3295400143 seconds\n",
      "epoc 9900: 19112.8\n",
      "100 epochs takes 49.7660021782 seconds\n",
      "epoc 10000: 18915.4\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid100_orig=train(data100, num_dims=30, num_epoc=10000, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the embedding on the densest 1000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 571.  319.  282. ...,  484.  304.  330.]\n",
      "<NDArray 1004 @cpu(0)>\n",
      "(81306L, 1004L)\n"
     ]
    }
   ],
   "source": [
    "sp_data1000 = get_densest(spm, 1000)\n",
    "data1000 = mx.ndarray.sparse.csr_matrix(sp_data1000)\n",
    "print(mx.ndarray.sum(data1000, axis=0))\n",
    "print(data1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svd error: 203856.0\n"
     ]
    }
   ],
   "source": [
    "U, s, Vh = sp.sparse.linalg.svds(sp_data1000, k=100)\n",
    "res = np.dot(sp_data1000.dot(Vh.T), Vh)\n",
    "print(\"svd error: \" + str(np.sum(np.square(res - sp_data1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old inputs: 100\n",
      "old hidden: 30\n",
      "new inputs: 1004\n",
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 10000\n",
      "reinit optimizer. New learning rate: 0.2\n",
      "100 epochs takes 432.728415012 seconds\n",
      "epoc 100: 494267.0\n",
      "100 epochs takes 432.248641968 seconds\n",
      "epoc 200: 425895.0\n",
      "100 epochs takes 429.028847933 seconds\n",
      "epoc 300: 397958.0\n",
      "100 epochs takes 431.694869995 seconds\n",
      "epoc 400: 381204.0\n",
      "100 epochs takes 428.774977922 seconds\n",
      "epoc 500: 370016.0\n",
      "100 epochs takes 435.297715902 seconds\n",
      "epoc 600: 361989.0\n",
      "100 epochs takes 432.369880199 seconds\n",
      "epoc 700: 355645.0\n",
      "100 epochs takes 432.517203093 seconds\n",
      "epoc 800: 350544.0\n",
      "100 epochs takes 432.737567902 seconds\n",
      "epoc 900: 346421.0\n",
      "100 epochs takes 431.994055033 seconds\n",
      "epoc 1000: 342977.0\n",
      "100 epochs takes 433.733255863 seconds\n",
      "epoc 1100: 339984.0\n",
      "100 epochs takes 434.355044842 seconds\n",
      "epoc 1200: 337325.0\n",
      "100 epochs takes 432.226321936 seconds\n",
      "epoc 1300: 334932.0\n",
      "100 epochs takes 434.114796877 seconds\n",
      "epoc 1400: 332744.0\n",
      "100 epochs takes 434.001944065 seconds\n",
      "epoc 1500: 330707.0\n",
      "100 epochs takes 432.858433962 seconds\n",
      "epoc 1600: 328790.0\n",
      "100 epochs takes 432.28443408 seconds\n",
      "epoc 1700: 326976.0\n",
      "100 epochs takes 429.506348133 seconds\n",
      "epoc 1800: 325254.0\n",
      "100 epochs takes 430.574714899 seconds\n",
      "epoc 1900: 323625.0\n",
      "100 epochs takes 433.045159101 seconds\n",
      "epoc 2000: 322088.0\n",
      "100 epochs takes 431.664439917 seconds\n",
      "epoc 2100: 320637.0\n",
      "100 epochs takes 430.560851097 seconds\n",
      "epoc 2200: 319266.0\n",
      "100 epochs takes 430.117973089 seconds\n",
      "epoc 2300: 317969.0\n",
      "100 epochs takes 432.501918077 seconds\n",
      "epoc 2400: 316740.0\n",
      "100 epochs takes 432.797761917 seconds\n",
      "epoc 2500: 315572.0\n",
      "100 epochs takes 430.653242111 seconds\n",
      "epoc 2600: 314460.0\n",
      "100 epochs takes 433.721477032 seconds\n",
      "epoc 2700: 313400.0\n",
      "100 epochs takes 431.793606043 seconds\n",
      "epoc 2800: 312386.0\n",
      "100 epochs takes 430.891934872 seconds\n",
      "epoc 2900: 311417.0\n",
      "100 epochs takes 433.106526852 seconds\n",
      "epoc 3000: 310486.0\n",
      "100 epochs takes 434.357372046 seconds\n",
      "epoc 3100: 309590.0\n",
      "100 epochs takes 432.666631937 seconds\n",
      "epoc 3200: 308728.0\n",
      "100 epochs takes 432.282385111 seconds\n",
      "epoc 3300: 307894.0\n",
      "100 epochs takes 428.360591888 seconds\n",
      "epoc 3400: 307085.0\n",
      "100 epochs takes 429.801764011 seconds\n",
      "epoc 3500: 306300.0\n",
      "100 epochs takes 432.043131113 seconds\n",
      "epoc 3600: 305537.0\n",
      "100 epochs takes 431.25249815 seconds\n",
      "epoc 3700: 304791.0\n",
      "100 epochs takes 431.598446131 seconds\n",
      "epoc 3800: 304063.0\n",
      "100 epochs takes 429.133488894 seconds\n",
      "epoc 3900: 303351.0\n",
      "100 epochs takes 432.519405127 seconds\n",
      "epoc 4000: 302653.0\n",
      "100 epochs takes 432.646189928 seconds\n",
      "epoc 4100: 301965.0\n",
      "100 epochs takes 432.757914066 seconds\n",
      "epoc 4200: 301289.0\n",
      "100 epochs takes 429.369370937 seconds\n",
      "epoc 4300: 300623.0\n",
      "100 epochs takes 431.69540906 seconds\n",
      "epoc 4400: 299964.0\n",
      "100 epochs takes 428.59487319 seconds\n",
      "epoc 4500: 299312.0\n",
      "100 epochs takes 433.454143047 seconds\n",
      "epoc 4600: 298666.0\n",
      "100 epochs takes 432.216603041 seconds\n",
      "epoc 4700: 298026.0\n",
      "100 epochs takes 428.786484003 seconds\n",
      "epoc 4800: 297389.0\n",
      "100 epochs takes 431.723551035 seconds\n",
      "epoc 4900: 296754.0\n",
      "100 epochs takes 428.444703817 seconds\n",
      "epoc 5000: 296118.0\n",
      "100 epochs takes 429.642598867 seconds\n",
      "epoc 5100: 295483.0\n",
      "100 epochs takes 432.168067932 seconds\n",
      "epoc 5200: 294845.0\n",
      "100 epochs takes 433.221278906 seconds\n",
      "epoc 5300: 294204.0\n",
      "100 epochs takes 431.640995979 seconds\n",
      "epoc 5400: 293558.0\n",
      "100 epochs takes 433.281181097 seconds\n",
      "epoc 5500: 292904.0\n",
      "100 epochs takes 433.161706209 seconds\n",
      "epoc 5600: 292243.0\n",
      "100 epochs takes 432.329707861 seconds\n",
      "epoc 5700: 291574.0\n",
      "100 epochs takes 432.57375288 seconds\n",
      "epoc 5800: 290896.0\n",
      "100 epochs takes 432.29885006 seconds\n",
      "epoc 5900: 290207.0\n",
      "100 epochs takes 434.109987974 seconds\n",
      "epoc 6000: 289507.0\n",
      "100 epochs takes 433.293788195 seconds\n",
      "epoc 6100: 288798.0\n",
      "100 epochs takes 435.009533167 seconds\n",
      "epoc 6200: 288076.0\n",
      "100 epochs takes 431.497982979 seconds\n",
      "epoc 6300: 287345.0\n",
      "100 epochs takes 432.503038883 seconds\n",
      "epoc 6400: 286605.0\n",
      "100 epochs takes 431.248731852 seconds\n",
      "epoc 6500: 285861.0\n",
      "100 epochs takes 432.077907085 seconds\n",
      "epoc 6600: 285110.0\n",
      "100 epochs takes 434.20498395 seconds\n",
      "epoc 6700: 284362.0\n",
      "100 epochs takes 431.428271055 seconds\n",
      "epoc 6800: 283616.0\n",
      "100 epochs takes 434.759538174 seconds\n",
      "epoc 6900: 282877.0\n",
      "100 epochs takes 431.1762321 seconds\n",
      "epoc 7000: 282149.0\n",
      "100 epochs takes 430.921285868 seconds\n",
      "epoc 7100: 281433.0\n",
      "100 epochs takes 433.736908913 seconds\n",
      "epoc 7200: 280731.0\n",
      "100 epochs takes 429.174473047 seconds\n",
      "epoc 7300: 280044.0\n",
      "100 epochs takes 431.914810896 seconds\n",
      "epoc 7400: 279374.0\n",
      "100 epochs takes 432.503262043 seconds\n",
      "epoc 7500: 278722.0\n",
      "100 epochs takes 431.712754965 seconds\n",
      "epoc 7600: 278084.0\n",
      "100 epochs takes 428.467554092 seconds\n",
      "epoc 7700: 277462.0\n",
      "100 epochs takes 429.010721922 seconds\n",
      "epoc 7800: 276856.0\n",
      "100 epochs takes 428.525381088 seconds\n",
      "epoc 7900: 276265.0\n",
      "100 epochs takes 431.851495028 seconds\n",
      "epoc 8000: 275689.0\n",
      "100 epochs takes 431.678200006 seconds\n",
      "epoc 8100: 275126.0\n",
      "100 epochs takes 428.050547123 seconds\n",
      "epoc 8200: 274578.0\n",
      "100 epochs takes 430.748335123 seconds\n",
      "epoc 8300: 274042.0\n",
      "100 epochs takes 434.569108009 seconds\n",
      "epoc 8400: 273521.0\n",
      "100 epochs takes 432.278654814 seconds\n",
      "epoc 8500: 273011.0\n",
      "100 epochs takes 433.960632086 seconds\n",
      "epoc 8600: 272515.0\n",
      "100 epochs takes 431.846256971 seconds\n",
      "epoc 8700: 272029.0\n",
      "100 epochs takes 430.902433872 seconds\n",
      "epoc 8800: 271555.0\n",
      "100 epochs takes 431.373863935 seconds\n",
      "epoc 8900: 271091.0\n",
      "100 epochs takes 431.290797949 seconds\n",
      "epoc 9000: 270637.0\n",
      "100 epochs takes 432.708992004 seconds\n",
      "epoc 9100: 270193.0\n",
      "100 epochs takes 431.247102976 seconds\n",
      "epoc 9200: 269757.0\n",
      "100 epochs takes 431.937280178 seconds\n",
      "epoc 9300: 269330.0\n",
      "100 epochs takes 435.351942778 seconds\n",
      "epoc 9400: 268911.0\n",
      "100 epochs takes 433.345110178 seconds\n",
      "epoc 9500: 268502.0\n",
      "100 epochs takes 429.20625782 seconds\n",
      "epoc 9600: 268098.0\n",
      "100 epochs takes 431.262514114 seconds\n",
      "epoc 9700: 267702.0\n",
      "100 epochs takes 428.808784962 seconds\n",
      "epoc 9800: 267314.0\n",
      "100 epochs takes 430.903738976 seconds\n",
      "epoc 9900: 266931.0\n",
      "100 epochs takes 433.47507596 seconds\n",
      "epoc 10000: 266555.0\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid1000_init = extend_params(params_sigmoid100, sp_data1000, 100, rand_init=True)\n",
    "model = AutoEncoderModel(data1000, num_dims=100, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)\n",
    "params_sigmoid1000 = model.train(data1000, num_epoc=10000, params=params_sigmoid1000_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal activation: tanh\n",
      "Output activation: sigmoid\n",
      "Learning rate: 0.2\n",
      "batch size: 2000\n",
      "internal #epochs: 10000\n",
      "100 epochs takes 431.080384016 seconds\n",
      "epoc 100: 508423.0\n",
      "100 epochs takes 431.098898172 seconds\n",
      "epoc 200: 460172.0\n",
      "100 epochs takes 430.493998051 seconds\n",
      "epoc 300: 447464.0\n",
      "100 epochs takes 430.86473608 seconds\n",
      "epoc 400: 441789.0\n",
      "100 epochs takes 429.94924593 seconds\n",
      "epoc 500: 438611.0\n",
      "100 epochs takes 431.865408182 seconds\n",
      "epoc 600: 436578.0\n",
      "100 epochs takes 434.106994867 seconds\n",
      "epoc 700: 435149.0\n",
      "100 epochs takes 433.679186821 seconds\n",
      "epoc 800: 434061.0\n",
      "100 epochs takes 433.638868093 seconds\n",
      "epoc 900: 433171.0\n",
      "100 epochs takes 432.451326847 seconds\n",
      "epoc 1000: 432383.0\n",
      "100 epochs takes 436.009906054 seconds\n",
      "epoc 1100: 431612.0\n",
      "100 epochs takes 433.658218861 seconds\n",
      "epoc 1200: 430779.0\n",
      "100 epochs takes 432.428790092 seconds\n",
      "epoc 1300: 429789.0\n",
      "100 epochs takes 435.449054956 seconds\n",
      "epoc 1400: 428567.0\n",
      "100 epochs takes 434.815099001 seconds\n",
      "epoc 1500: 427132.0\n",
      "100 epochs takes 432.409188986 seconds\n",
      "epoc 1600: 425567.0\n",
      "100 epochs takes 431.160838127 seconds\n",
      "epoc 1700: 423828.0\n",
      "100 epochs takes 431.237700939 seconds\n",
      "epoc 1800: 421807.0\n",
      "100 epochs takes 431.675616026 seconds\n",
      "epoc 1900: 419493.0\n",
      "100 epochs takes 436.407316208 seconds\n",
      "epoc 2000: 416947.0\n",
      "100 epochs takes 434.099112988 seconds\n",
      "epoc 2100: 414292.0\n",
      "100 epochs takes 433.66873908 seconds\n",
      "epoc 2200: 411657.0\n",
      "100 epochs takes 433.874003172 seconds\n",
      "epoc 2300: 409143.0\n",
      "100 epochs takes 432.134377003 seconds\n",
      "epoc 2400: 406821.0\n",
      "100 epochs takes 432.361398935 seconds\n",
      "epoc 2500: 404760.0\n",
      "100 epochs takes 436.093175888 seconds\n",
      "epoc 2600: 402959.0\n",
      "100 epochs takes 433.009018898 seconds\n",
      "epoc 2700: 401334.0\n",
      "100 epochs takes 431.750756979 seconds\n",
      "epoc 2800: 399799.0\n",
      "100 epochs takes 431.653563023 seconds\n",
      "epoc 2900: 398300.0\n",
      "100 epochs takes 434.080024958 seconds\n",
      "epoc 3000: 396815.0\n",
      "100 epochs takes 436.16879797 seconds\n",
      "epoc 3100: 395334.0\n",
      "100 epochs takes 434.233736038 seconds\n",
      "epoc 3200: 393871.0\n",
      "100 epochs takes 434.101132154 seconds\n",
      "epoc 3300: 392440.0\n",
      "100 epochs takes 433.141485214 seconds\n",
      "epoc 3400: 391063.0\n",
      "100 epochs takes 432.07354188 seconds\n",
      "epoc 3500: 389759.0\n",
      "100 epochs takes 431.03391099 seconds\n",
      "epoc 3600: 388541.0\n",
      "100 epochs takes 431.436028004 seconds\n",
      "epoc 3700: 387412.0\n",
      "100 epochs takes 432.441930056 seconds\n",
      "epoc 3800: 386371.0\n",
      "100 epochs takes 433.084938049 seconds\n",
      "epoc 3900: 385410.0\n",
      "100 epochs takes 433.481759071 seconds\n",
      "epoc 4000: 384520.0\n",
      "100 epochs takes 434.834981203 seconds\n",
      "epoc 4100: 383690.0\n",
      "100 epochs takes 436.1044209 seconds\n",
      "epoc 4200: 382909.0\n",
      "100 epochs takes 432.618688107 seconds\n",
      "epoc 4300: 382170.0\n",
      "100 epochs takes 433.563745975 seconds\n",
      "epoc 4400: 381464.0\n",
      "100 epochs takes 430.864768982 seconds\n",
      "epoc 4500: 380787.0\n",
      "100 epochs takes 433.132112026 seconds\n",
      "epoc 4600: 380132.0\n",
      "100 epochs takes 430.408668995 seconds\n",
      "epoc 4700: 379497.0\n",
      "100 epochs takes 429.16891408 seconds\n",
      "epoc 4800: 378875.0\n",
      "100 epochs takes 429.471240997 seconds\n",
      "epoc 4900: 378266.0\n",
      "100 epochs takes 435.164875984 seconds\n",
      "epoc 5000: 377668.0\n",
      "100 epochs takes 432.489019871 seconds\n",
      "epoc 5100: 377075.0\n",
      "100 epochs takes 431.804640055 seconds\n",
      "epoc 5200: 376489.0\n",
      "100 epochs takes 432.999241829 seconds\n",
      "epoc 5300: 375908.0\n",
      "100 epochs takes 432.373239994 seconds\n",
      "epoc 5400: 375329.0\n",
      "100 epochs takes 431.70733881 seconds\n",
      "epoc 5500: 374754.0\n",
      "100 epochs takes 433.876001835 seconds\n",
      "epoc 5600: 374179.0\n",
      "100 epochs takes 433.464462996 seconds\n",
      "epoc 5700: 373606.0\n",
      "100 epochs takes 432.510556936 seconds\n",
      "epoc 5800: 373033.0\n",
      "100 epochs takes 435.129097939 seconds\n",
      "epoc 5900: 372458.0\n",
      "100 epochs takes 433.831048012 seconds\n",
      "epoc 6000: 371882.0\n",
      "100 epochs takes 432.495541811 seconds\n",
      "epoc 6100: 371304.0\n",
      "100 epochs takes 432.166503906 seconds\n",
      "epoc 6200: 370725.0\n",
      "100 epochs takes 433.476287842 seconds\n",
      "epoc 6300: 370145.0\n",
      "100 epochs takes 429.66824007 seconds\n",
      "epoc 6400: 369561.0\n",
      "100 epochs takes 431.268422127 seconds\n",
      "epoc 6500: 368975.0\n",
      "100 epochs takes 429.803611994 seconds\n",
      "epoc 6600: 368387.0\n",
      "100 epochs takes 430.67910099 seconds\n",
      "epoc 6700: 367795.0\n",
      "100 epochs takes 430.126095057 seconds\n",
      "epoc 6800: 367201.0\n",
      "100 epochs takes 432.847745895 seconds\n",
      "epoc 6900: 366603.0\n",
      "100 epochs takes 432.028486013 seconds\n",
      "epoc 7000: 366001.0\n",
      "100 epochs takes 430.746886969 seconds\n",
      "epoc 7100: 365395.0\n",
      "100 epochs takes 434.205336809 seconds\n",
      "epoc 7200: 364786.0\n",
      "100 epochs takes 431.500965834 seconds\n",
      "epoc 7300: 364172.0\n",
      "100 epochs takes 432.183676958 seconds\n",
      "epoc 7400: 363551.0\n",
      "100 epochs takes 432.418908834 seconds\n",
      "epoc 7500: 362925.0\n",
      "100 epochs takes 430.68129015 seconds\n",
      "epoc 7600: 362293.0\n",
      "100 epochs takes 432.460183144 seconds\n",
      "epoc 7700: 361654.0\n",
      "100 epochs takes 431.224821091 seconds\n",
      "epoc 7800: 361008.0\n",
      "100 epochs takes 437.16405201 seconds\n",
      "epoc 7900: 360355.0\n",
      "100 epochs takes 434.738861799 seconds\n",
      "epoc 8000: 359695.0\n",
      "100 epochs takes 436.4133389 seconds\n",
      "epoc 8100: 359025.0\n",
      "100 epochs takes 435.330188036 seconds\n",
      "epoc 8200: 358347.0\n",
      "100 epochs takes 434.075345039 seconds\n",
      "epoc 8300: 357660.0\n",
      "100 epochs takes 435.634904146 seconds\n",
      "epoc 8400: 356966.0\n",
      "100 epochs takes 435.783385038 seconds\n",
      "epoc 8500: 356264.0\n",
      "100 epochs takes 433.860846043 seconds\n",
      "epoc 8600: 355554.0\n",
      "100 epochs takes 436.685934067 seconds\n",
      "epoc 8700: 354836.0\n",
      "100 epochs takes 430.493324995 seconds\n",
      "epoc 8800: 354111.0\n",
      "100 epochs takes 431.474987984 seconds\n",
      "epoc 8900: 353382.0\n",
      "100 epochs takes 431.223245144 seconds\n",
      "epoc 9000: 352647.0\n",
      "100 epochs takes 431.352371931 seconds\n",
      "epoc 9100: 351908.0\n",
      "100 epochs takes 433.543335915 seconds\n",
      "epoc 9200: 351167.0\n",
      "100 epochs takes 431.604271889 seconds\n",
      "epoc 9300: 350425.0\n",
      "100 epochs takes 432.691607952 seconds\n",
      "epoc 9400: 349684.0\n",
      "100 epochs takes 435.581202984 seconds\n",
      "epoc 9500: 348946.0\n",
      "100 epochs takes 434.306548119 seconds\n",
      "epoc 9600: 348209.0\n",
      "100 epochs takes 435.304809093 seconds\n",
      "epoc 9700: 347477.0\n",
      "100 epochs takes 434.264694929 seconds\n",
      "epoc 9800: 346748.0\n",
      "100 epochs takes 436.751240015 seconds\n",
      "epoc 9900: 346025.0\n",
      "100 epochs takes 434.08788991 seconds\n",
      "epoc 10000: 345305.0\n"
     ]
    }
   ],
   "source": [
    "params_sigmoid1000=train(data1000, num_dims=100, num_epoc=10000, internal_act='tanh', output_act='sigmoid', learning_rate=0.2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1000 = mx.ndarray.sparse.csr_matrix(sp_data1000)\n",
    "print(mx.ndarray.sum(data1000, axis=0))\n",
    "print(data1000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_matrix = [[0.9, 0.1], [0.1, 0.9]]\n",
    "block_sizes = [70, 30]\n",
    "g = ig.Graph.SBM(100, pref_matrix, block_sizes, directed=True)\n",
    "sim_spm = g.get_adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
